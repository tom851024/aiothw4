import os

# --- 0. è¨­å®šç’°å¢ƒè®Šæ•¸ (éš±è— TensorFlow/OneDNN è­¦å‘Š) ---
# âš ï¸ æ³¨æ„ï¼šé€™å¿…é ˆå¯«åœ¨æ‰€æœ‰ import ä¹‹å‰æ‰æœ‰æ•ˆ
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import streamlit as st
import aisuite as ai
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from huggingface_hub import login

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="åœ‹éš›æ–°è AI ç·¨è­¯", page_icon="ğŸ“°")
st.title("ğŸ“° åœ‹éš›æ–°è AI ç·¨è­¯åŠ©æ‰‹")
st.caption("ä½¿ç”¨ RAG æŠ€è¡“èˆ‡ Embedding Gemma æ¨¡å‹")

# --- 2. è™•ç† Secrets (é‡‘é‘°) ---
try:
    # å˜—è©¦å¾ Streamlit secrets è®€å– (æœ¬åœ°ç«¯è®€å– .streamlit/secrets.toml)
    hf_token = st.secrets["HF_TOKEN"]
    groq_api_key = st.secrets["GROQ_API_KEY"]
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸
    os.environ['GROQ_API_KEY'] = groq_api_key
    
    # â¬‡ï¸ [é—œéµä¿®æ­£] å¿…é ˆæ˜ç¢ºè¨­å®šé€™å€‹ç’°å¢ƒè®Šæ•¸ï¼Œä¸‹é¢çš„ Embedding Class æ‰èƒ½è®€å–åˆ° Token
    os.environ['HuggingFaceHub_API_TOKEN'] = hf_token 
    
    login(token=hf_token)
except Exception as e:
    st.error("âŒ é‡‘é‘°æœªè¨­å®šï¼è«‹ç¢ºèª .streamlit/secrets.toml (æœ¬åœ°) æˆ– Streamlit Cloud Secrets è¨­å®šæ­£ç¢ºã€‚")
    st.stop()

# --- 3. å®šç¾©è‡ªè¨‚ Embeddings é¡åˆ¥ ---
class EmbeddingGemmaEmbeddings(HuggingFaceEmbeddings):
    def __init__(self, **kwargs):
        super().__init__(
            model_name="google/embeddinggemma-300m",
            encode_kwargs={"normalize_embeddings": True},
            model_kwargs={
                # â¬‡ï¸ [é—œéµä¿®æ­£] å…è¨±åŸ·è¡Œ Google çš„è‡ªå®šç¾©æ¨¡å‹ç¨‹å¼ç¢¼
                "trust_remote_code": True, 
                # â¬‡ï¸ [é—œéµä¿®æ­£] æ˜ç¢ºå‚³å…¥ Tokenï¼Œè§£æ±ºé›²ç«¯ç’°å¢ƒæ¬Šé™å•é¡Œ
                "token": os.environ.get("HuggingFaceHub_API_TOKEN") 
            },
            **kwargs
        )

    def embed_documents(self, texts):
        texts = [f"title: none | text: {t}" for t in texts]
        return super().embed_documents(texts)

    def embed_query(self, text):
        return super().embed_query(f"task: search result | query: {text}")

# --- 4. è¼‰å…¥è³‡æº (UI èˆ‡é‚è¼¯åˆ†é›¢ç‰ˆ) ---
# â¬‡ï¸ [é—œéµä¿®æ­£] ä½¿ç”¨ show_spinner è®“ Streamlit è‡ªå‹•è™•ç†è¼‰å…¥å‹•ç•«ï¼Œé¿å… CacheReplayClosureError
@st.cache_resource(show_spinner="ğŸ”„ ç³»çµ±åˆå§‹åŒ–ä¸­ï¼Œæ­£åœ¨è¼‰å…¥æ¨¡å‹èˆ‡è³‡æ–™åº« (åˆæ¬¡åŸ·è¡Œéœ€æ•¸åˆ†é˜)...")
def load_resources():
    # é€™è£¡é¢ã€Œåªç•™ printã€ï¼ŒæŠŠæ‰€æœ‰ st.info, st.empty å…¨éƒ¨æ‹¿æ‰
    # é€™æ¨£å°±ä¸æœƒå ± CacheReplayClosureError äº†

    print("\n" + "="*50)
    print("ğŸš€ ç³»çµ±å•Ÿå‹•ä¸­...")

    # --- A. è¼‰å…¥ Embedding ---
    print("â³ Step 1: æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
    embedding_model = EmbeddingGemmaEmbeddings()
    print("âœ… Step 1: Embedding æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

    # --- B. è¼‰å…¥ FAISS è³‡æ–™åº« ---
    print("â³ Step 2: æ­£åœ¨è®€å– FAISS å‘é‡è³‡æ–™åº«...")
    if not os.path.exists("faiss_db"):
        # é€™è£¡æ”¹ç”¨ raise Exceptionï¼Œè®“å¤–å±¤å»æŠ“éŒ¯èª¤ï¼Œä¸è¦åœ¨å¿«å–å…§ç”¨ st.error
        raise FileNotFoundError("æ‰¾ä¸åˆ° faiss_db è³‡æ–™å¤¾ï¼è«‹ç¢ºèªå·²å°‡è³‡æ–™å¤¾æ”¾å…¥å°ˆæ¡ˆç›®éŒ„ã€‚")

    vectorstore = FAISS.load_local(
        "faiss_db",
        embeddings=embedding_model,
        allow_dangerous_deserialization=True
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    print("âœ… Step 2: FAISS è³‡æ–™åº«è®€å–å®Œæˆï¼")

    # --- C. åˆå§‹åŒ– LLM Client ---
    print("â³ Step 3: åˆå§‹åŒ– LLM Client...")
    client = ai.Client()
    print("âœ… Step 3: LLM Client æº–å‚™å°±ç·’ï¼")
    print("="*50 + "\n")

    return retriever, client

# åŸ·è¡Œè¼‰å…¥ (å¦‚æœå¡ä½ï¼Œè«‹çœ‹çµ‚ç«¯æ©Ÿ)
try:
    retriever, client = load_resources()
except Exception as e:
    st.error(f"âŒ ç³»çµ±è¼‰å…¥å¤±æ•—: {e}")
    st.stop()

# --- 5. å®šç¾© Prompt èˆ‡ç”Ÿæˆé‚è¼¯ ---
system_prompt = "ä½ æ˜¯è³‡æ·±çš„åœ‹éš›æ–°èç·¨è­¯ï¼Œå°ˆé–€è² è²¬å°‡å¤–é›»è³‡è¨Šæ•´ç†æˆå°ç£è®€è€…å®¹æ˜“ç†è§£çš„å ±å°ã€‚è«‹ä¿æŒå®¢è§€ã€å°ˆæ¥­ã€ç²¾ç°¡çš„èªæ°£ï¼Œä¸¦ä½¿ç”¨å°ç£æ…£ç”¨çš„ç¿»è­¯åè©ï¼ˆä¾‹å¦‚ï¼šé›ªæ¢¨è€Œéæ‚‰å°¼ã€ç´è¥¿è˜­è€Œéæ–°è¥¿è˜­ï¼‰ï¼Œä¸¦ç”¨å°ç£æ…£ç”¨çš„ä¸­æ–‡å›æ‡‰ã€‚"

prompt_template = """
è«‹åƒè€ƒä¸‹åˆ—æ–°èè³‡æ–™ç‰‡æ®µï¼š
{retrieved_chunks}

è®€è€…æå•ï¼š{question}

è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™æ’°å¯«å›æ‡‰ï¼š
1. é‡é»æ‘˜è¦ï¼šç›´æ¥é‡å°å•é¡Œå›ç­”æ ¸å¿ƒäº‹å¯¦ï¼ˆäººäº‹æ™‚åœ°ç‰©ï¼‰ã€‚
2. è‹¥ä¸Šè¿°è³‡æ–™ç„¡æ³•å®Œæ•´å›ç­”å•é¡Œï¼Œè«‹èª å¯¦å‘ŠçŸ¥è³‡è¨Šä¸è¶³ï¼Œä¸¦å»ºè­°è®€è€…æŸ¥é–±ã€ŒBBCã€ã€ã€ŒCNNã€æˆ–ã€Œä¸­å¤®ç¤¾ã€ç­‰æ¬Šå¨åª’é«”ä»¥ç²å–æœ€æ–°æ¶ˆæ¯ã€‚

å›æ‡‰å…§å®¹ï¼š
"""

def chat_with_rag(user_input):
    # 1. æª¢ç´¢
    docs = retriever.invoke(user_input)
    retrieved_chunks = "\n\n".join([doc.page_content for doc in docs])

    # 2. çµ„åˆ Prompt
    final_prompt = prompt_template.format(retrieved_chunks=retrieved_chunks, question=user_input)

    # 3. å‘¼å« LLM
    # è‹¥ Groq æœ‰æ›´æ–°æ¨¡å‹åç¨±ï¼Œè«‹åœ¨æ­¤èª¿æ•´
    model_name = "groq:llama-3.3-70b-versatile" 
    
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": final_prompt},
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âš ï¸ å‘¼å« LLM æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

# --- 6. èŠå¤©ä»‹é¢ (UI) ---
if "messages" not in st.session_state:
    welcome_msg = """
    ä½ å¥½ï¼æˆ‘æ˜¯åœ‹éš›æ–°è AI ç¿»è­¯åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ç›®å‰çš„åœ‹éš›æ–°èé‡é»æ‘˜è¦ï¼š

    1. **ä¿„ç¾…æ–¯ç©ºè¥²è¥¿çƒå…‹è˜­**ï¼šé€ æˆè‡³å°‘ 28 äººæ­»å‚·ï¼Œæ•‘æ´è¡Œå‹•æŒçºŒé€²è¡Œä¸­ã€‚
    2. **ç¾åœ‹åœæ­¢ç”Ÿç”¢ä¸€åˆ†ç¡¬å¹£**ï¼šå› æˆæœ¬å¢åŠ ï¼Œäº”åˆ†ç¡¬å¹£å¯èƒ½æˆç‚ºä¸‹ä¸€å€‹ç›®æ¨™ã€‚
    3. **é˜²ç–«åé …å»ºè­°**ï¼šåŒ…æ‹¬è¨­ç«‹æ‡‰æ€¥æ©Ÿæ§‹ã€å®šæœŸæ¼”ç·´ã€ç°¡åŒ–çµæ§‹åŠæ”¹å–„æ•¸æ“šç³»çµ±ã€‚
    4. **æˆ°ç«ä¸‹çš„çƒå…‹è˜­é…’èŠ**ï¼šè£½é€ å•†åŠªåŠ›åœ¨æˆ°çˆ­ä¸­ä¿å­˜å‚³çµ±ã€‚
    5. **ã€Šå¤æ´›ç‰¹çš„ç¶²ã€‹æ”¿æ²»åŒ–çˆ­è­°**ï¼šä½œè€…å­«å¥³å°æ­¤è¡¨ç¤ºæ‰¹è©•ã€‚
    6. **IISS æŒ‡æ§ä¿„ç¾…æ–¯æ··åˆæˆ°**ï¼šç ”ç©¶æŒ‡å‡ºä¿„ç¾…æ–¯æ­£é€²è¡Œç ´å£ã€é–“è«œç­‰è¡Œå‹•ä»¥å‹•æ–æ­æ´²ç©©å®šã€‚

    ä½ å¯ä»¥é‡å°ä»¥ä¸Šæ–°èåšæ›´æ·±å…¥çš„æå•ï¼Œè‹¥è³‡æ–™åº«ä¸­æœ‰ç›¸é—œå ±å°ï¼Œæˆ‘å°‡ç‚ºä½ è©³ç´°è§£ç­”ï¼
    """
    st.session_state.messages = [{"role": "assistant", "content": welcome_msg}]

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è¼¸å…¥æ¡†
if prompt := st.chat_input("è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„åœ‹éš›æ–°è..."):
    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # é¡¯ç¤º AI å›æ‡‰
    with st.chat_message("assistant"):
        with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢è³‡æ–™ä¸¦æ’°å¯«å ±å°..."):
            response = chat_with_rag(prompt)
            st.markdown(response)
            
    # å„²å­˜å›æ‡‰
    st.session_state.messages.append({"role": "assistant", "content": response})