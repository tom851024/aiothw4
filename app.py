import os

# --- 0. è¨­å®šç’°å¢ƒè®Šæ•¸ (éš±è— TensorFlow/OneDNN è­¦å‘Š) ---
# âš ï¸ æ³¨æ„ï¼šé€™å¿…é ˆå¯«åœ¨æ‰€æœ‰ import ä¹‹å‰æ‰æœ‰æ•ˆ
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import streamlit as st
import aisuite as ai
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from huggingface_hub import login

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="åœ‹éš›æ–°è AI ç·¨è­¯", page_icon="ğŸ“°")
st.title("ğŸ“° åœ‹éš›æ–°è AI ç·¨è­¯åŠ©æ‰‹")
st.caption("ä½¿ç”¨ RAG æŠ€è¡“èˆ‡ Embedding Gemma æ¨¡å‹")

# --- 2. è™•ç† Secrets (é‡‘é‘°) ---
try:
    # å˜—è©¦å¾ Streamlit secrets è®€å– (æœ¬åœ°ç«¯è®€å– .streamlit/secrets.toml)
    hf_token = st.secrets["HF_TOKEN"]
    groq_api_key = st.secrets["GROQ_API_KEY"]
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸
    os.environ['GROQ_API_KEY'] = groq_api_key
    login(token=hf_token)
except Exception as e:
    st.error("âŒ é‡‘é‘°æœªè¨­å®šï¼è«‹ç¢ºèª .streamlit/secrets.toml (æœ¬åœ°) æˆ– Streamlit Cloud Secrets è¨­å®šæ­£ç¢ºã€‚")
    st.stop()

# --- 3. å®šç¾©è‡ªè¨‚ Embeddings é¡åˆ¥ ---
class EmbeddingGemmaEmbeddings(HuggingFaceEmbeddings):
    def __init__(self, **kwargs):
        super().__init__(
            model_name="google/embeddinggemma-300m",
            encode_kwargs={"normalize_embeddings": True},
            **kwargs
        )

    def embed_documents(self, texts):
        texts = [f"title: none | text: {t}" for t in texts]
        return super().embed_documents(texts)

    def embed_query(self, text):
        return super().embed_query(f"task: search result | query: {text}")

# --- 4. è¼‰å…¥è³‡æº (åŠ å…¥é€²åº¦æç¤º) ---
@st.cache_resource
def load_resources():
    # é€™è£¡é¢ã€Œåªç•™ printã€ï¼ŒæŠŠæ‰€æœ‰ st.info, st.empty å…¨éƒ¨æ‹¿æ‰
    # é€™æ¨£å°±ä¸æœƒå ± CacheReplayClosureError äº†

    print("\n" + "="*50)
    print("ğŸš€ ç³»çµ±å•Ÿå‹•ä¸­...")

    # --- A. è¼‰å…¥ Embedding ---
    print("â³ Step 1: æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
    embedding_model = EmbeddingGemmaEmbeddings()
    print("âœ… Step 1: Embedding æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

    # --- B. è¼‰å…¥ FAISS è³‡æ–™åº« ---
    print("â³ Step 2: æ­£åœ¨è®€å– FAISS å‘é‡è³‡æ–™åº«...")
    if not os.path.exists("faiss_db"):
        # é€™è£¡æ”¹ç”¨ raise Exceptionï¼Œè®“å¤–å±¤å»æŠ“éŒ¯èª¤ï¼Œä¸è¦åœ¨å¿«å–å…§ç”¨ st.error
        raise FileNotFoundError("âŒ æ‰¾ä¸åˆ° faiss_db è³‡æ–™å¤¾ï¼è«‹ç¢ºèªå·²å°‡è³‡æ–™å¤¾æ”¾å…¥å°ˆæ¡ˆç›®éŒ„ã€‚")

    vectorstore = FAISS.load_local(
        "faiss_db",
        embeddings=embedding_model,
        allow_dangerous_deserialization=True
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    print("âœ… Step 2: FAISS è³‡æ–™åº«è®€å–å®Œæˆï¼")

    # --- C. åˆå§‹åŒ– LLM Client ---
    print("â³ Step 3: åˆå§‹åŒ– LLM Client...")
    client = ai.Client()
    print("âœ… Step 3: LLM Client æº–å‚™å°±ç·’ï¼")
    print("="*50 + "\n")

    return retriever, client

# åŸ·è¡Œè¼‰å…¥ (å¦‚æœå¡ä½ï¼Œè«‹çœ‹çµ‚ç«¯æ©Ÿ)
retriever, client = load_resources()

# --- 5. å®šç¾© Prompt èˆ‡ç”Ÿæˆé‚è¼¯ ---
system_prompt = "ä½ æ˜¯è³‡æ·±çš„åœ‹éš›æ–°èç·¨è­¯ï¼Œå°ˆé–€è² è²¬å°‡å¤–é›»è³‡è¨Šæ•´ç†æˆå°ç£è®€è€…å®¹æ˜“ç†è§£çš„å ±å°ã€‚è«‹ä¿æŒå®¢è§€ã€å°ˆæ¥­ã€ç²¾ç°¡çš„èªæ°£ï¼Œä¸¦ä½¿ç”¨å°ç£æ…£ç”¨çš„ç¿»è­¯åè©ï¼ˆä¾‹å¦‚ï¼šé›ªæ¢¨è€Œéæ‚‰å°¼ã€ç´è¥¿è˜­è€Œéæ–°è¥¿è˜­ï¼‰ï¼Œä¸¦ç”¨å°ç£æ…£ç”¨çš„ä¸­æ–‡å›æ‡‰ã€‚"

prompt_template = """
è«‹åƒè€ƒä¸‹åˆ—æ–°èè³‡æ–™ç‰‡æ®µï¼š
{retrieved_chunks}

è®€è€…æå•ï¼š{question}

è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™æ’°å¯«å›æ‡‰ï¼š
1. é‡é»æ‘˜è¦ï¼šç›´æ¥é‡å°å•é¡Œå›ç­”æ ¸å¿ƒäº‹å¯¦ï¼ˆäººäº‹æ™‚åœ°ç‰©ï¼‰ã€‚
2. è‹¥ä¸Šè¿°è³‡æ–™ç„¡æ³•å®Œæ•´å›ç­”å•é¡Œï¼Œè«‹èª å¯¦å‘ŠçŸ¥è³‡è¨Šä¸è¶³ï¼Œä¸¦å»ºè­°è®€è€…æŸ¥é–±ã€ŒBBCã€ã€ã€ŒCNNã€æˆ–ã€Œä¸­å¤®ç¤¾ã€ç­‰æ¬Šå¨åª’é«”ä»¥ç²å–æœ€æ–°æ¶ˆæ¯ã€‚

å›æ‡‰å…§å®¹ï¼š
"""

def chat_with_rag(user_input):
    # 1. æª¢ç´¢
    docs = retriever.invoke(user_input)
    retrieved_chunks = "\n\n".join([doc.page_content for doc in docs])

    # 2. çµ„åˆ Prompt
    final_prompt = prompt_template.format(retrieved_chunks=retrieved_chunks, question=user_input)

    # 3. å‘¼å« LLM
    model_name = "groq:llama-3.3-70b-versatile" 
    
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": final_prompt},
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âš ï¸ å‘¼å« LLM æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

# --- 6. èŠå¤©ä»‹é¢ (UI) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„åœ‹éš›æ–°è..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢è³‡æ–™ä¸¦æ’°å¯«å ±å°..."):
            response = chat_with_rag(prompt)
            st.markdown(response)
            
    st.session_state.messages.append({"role": "assistant", "content": response})